"use strict";(self.webpackChunkknowledge_base=self.webpackChunkknowledge_base||[]).push([[1739],{9509:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"market-data","metadata":{"permalink":"/knowledge-base/market-data","source":"@site/blog/2025-08-07-market-data.md","title":"Market Data","description":"Market data symbology refers to the system of codes and symbols to uniquely identify financial instruments and their attributes.","date":"2025-08-07T00:00:00.000Z","tags":[{"inline":false,"label":"Interview","permalink":"/knowledge-base/tags/interview","description":"Interview Preparation and Tips"}],"readingTime":2.025,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"market-data","title":"Market Data","authors":["hinnytsang"],"tags":["interview"]},"unlisted":false,"nextItem":{"title":"Questions to Ask the Interviewer","permalink":"/knowledge-base/questions-to-interviewer-1"}},"content":"Market data symbology refers to the system of codes and symbols to uniquely identify financial instruments and their attributes.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Ticker Symbols\\n\\nAlphabetic codes used to uniquely identify securities on exchange, examples:\\n\\n- AAPL for Apple Inc.\\n- TSLA for Tesla Inc.\\n\\n## ISIN (International Securities Identification Number)\\n\\nA 12-character alphanumeric code that uniquely identifies a security, used globally. Example:\\n\\n- US0378331005 for Apple Inc.\\n- US88160R1014 for Tesla Inc.\\n\\n## CUSIP (Committee on Uniform Securities Identification Procedures)\\n\\nA 9-character alphanumeric code used in the *United States* to identify securities. Example:\\n\\n- 037833100 for Apple Inc.\\n- 88160R101 for Tesla Inc.\\n\\n## SEDOL (Stock Exchange Daily Official List)\\n\\nA 7-character alphanumeric code used in the *United Kingdom and Ireland* to identify securities. Example:\\n\\n- 0263494 for Apple Inc.\\n- 0734644 for Tesla Inc.\\n\\n\\n## FIGI (Financial Instrument Global Identifier)\\n\\nA 12-character alphanumeric code that uniquely identifies a financial instrument, used globally. Example:\\n\\n- BBG000B9XRY4 for Apple Inc.\\n- BBG000B9XRY5 for Tesla Inc.\\n\\n## RIC (Reuters Instrument Code)\\n\\nAn alphanumeric code used to identify financial instruments, primarily in the *Reuters* trading platform. Example:\\n\\n- AAPL.O for Apple Inc.\\n- TSLA.O for Tesla Inc.\\n\\n\\n# Summary of Market Data Identifiers\\n\\n| Identifier                         | Length    | Coverage          | Structure                                               | Primary Use              | Key Benefits                 | Limitations                |\\n| ---------------------------------- | --------- | ----------------- | ------------------------------------------------------- | ------------------------ | ---------------------------- | -------------------------- |\\n| **Ticker Symbol**                  | 1-5 chars | Exchange-specific | No specific structure                                   | Quick trading reference  | Simple, widely recognized    | Not globally unique        |\\n| **ISIN**                           | 12 chars  | Global            | Country(2) + NSIN(9) + *cd(1)                           | Cross-border trading     | Universal standard           | Complex calculation        |\\n| **CUSIP** (Q-sip)                  | 9 chars   | North America     | Issuer(6) + issue type(2) + cd(1)                       | US/Canada trading        | Widely used in North America | Regional only, proprietary |\\n| **SEDOL**                          | 7 chars   | UK/Ireland        | Alphanumeric no vowels + cd(1)                          | UK securities trading    | Efficient for UK market      | Limited geographic scope   |\\n| **FIGI**          (Formerly BBGID) | 12 chars  | Global            | Consonants(2) + \'G\' + Alphanumeric no vowels(8) + cd(1) | Bloomberg ecosystem      | Open-source, free            | Less widely adopted        |\\n| **RIC**                            | Variable  | Global            | Alphanumeric, often with exchange suffix                | Reuters trading platform | Widely used in Reuters       | Not standardized globally  |\\n\\nWhere:\\n\\n* cd: Check Digit\\n\\n# Summary of Market Data Identifiers"},{"id":"questions-to-interviewer-1","metadata":{"permalink":"/knowledge-base/questions-to-interviewer-1","source":"@site/blog/2025-08-03-questions-to-interviewer.md","title":"Questions to Ask the Interviewer","description":"Collections of the quesions.","date":"2025-08-03T00:00:00.000Z","tags":[{"inline":false,"label":"Interview","permalink":"/knowledge-base/tags/interview","description":"Interview Preparation and Tips"}],"readingTime":0.435,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"questions-to-interviewer-1","title":"Questions to Ask the Interviewer","authors":["hinnytsang"],"tags":["interview"]},"unlisted":false,"prevItem":{"title":"Market Data","permalink":"/knowledge-base/market-data"},"nextItem":{"title":"Finance Knowledge Notes","permalink":"/knowledge-base/finance-knowledges"}},"content":"Collections of the quesions.\\n\\n\x3c!-- truncate --\x3e\\n\\n1. Do you have any questions or concerns about my qualifications for this role?\\n    - If possitive: \\"Great! I look forward to the next steps in the interview process.\\"\\n    - If negative: Non defensive response:\\n        - Be honest.\\n        - Thank you so much, and I appreciate your feedback.\\n\\n2. What are the skills and behaviors of the most effecrive people on your team hear in XXX?\\n\\n3. What attracted you to work for this company, and what keeps you here?"},{"id":"finance-knowledges","metadata":{"permalink":"/knowledge-base/finance-knowledges","source":"@site/blog/2025-08-02-finance-knowledges.md","title":"Finance Knowledge Notes","description":"Below are some finance notes.","date":"2025-08-02T00:00:00.000Z","tags":[{"inline":true,"label":"finance","permalink":"/knowledge-base/tags/finance"}],"readingTime":1.14,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"finance-knowledges","title":"Finance Knowledge Notes","authors":["hinnytsang"],"tags":["finance"]},"unlisted":false,"prevItem":{"title":"Questions to Ask the Interviewer","permalink":"/knowledge-base/questions-to-interviewer-1"},"nextItem":{"title":"Probability Puzzles 1","permalink":"/knowledge-base/probability-puzzles-1"}},"content":"Below are some finance notes.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n### DV01\\n\\nDV01 (dollar value of 01) measures the change in the price of a bond for a 1 basis point (0.01%) change in yield.\\n\\n(Sensitivity of bond price to yield change)\\n\\n- Linear approximation of price sensitivity, non-linear if large yield changes.\\n- Used in fixed-income portfolio management.\\n\\nIt is calculated as:\\n```math\\nDV01 = -\\\\frac{\\\\Delta P}{\\\\Delta y} \\\\times 10000\\n```\\n\\n\\n*The dollar duration of a portfolio is the sum of the DV01 of each bond in the portfolio*.\\n\\n### Value at Risk (VaR)\\n\\n\\nGiven confidence level $\\\\alpha$, the VaR is defined as\\n\\n```math\\n\\\\alpha = \\\\int_{-VaR}^{\\\\infty} xf(x) dx\\n```\\n\\nwhere $x$ is the dollar profit (loss) and $f(x)$ is the probability density function.\\n\\nHowever, it is not-additive, i.e. if portfolio $C$ = $A + B$, it is not guaranteed that $VaR(C) < VaR(A) + VaR(B)$.\\n\\n\\n### Forward and Futures\\n\\nIn the interest rate is deterministics, the forwards and futures prices are equivalent.\\n\\n```math\\nF = S_0 e^{(r + u - y)\\\\tau},\\n```\\n\\nwhere $u$ is the storages cost, $y$ is the divident yield for investment assets, covenience yield for commodities, and foreign risk-free interest rate for foreign exchange.\\n\\n\\n### Interest Rate Model\\n\\nTwo categories:\\n\\n1. Short rate models\\n    - Evolution of instantaneous interest rate is stochastic.\\n\\n2. Forward rate models\\n\\n\\nAnother classification\\n\\n1. Arbitrage-free models\\n\\n2. Equilibrium models"},{"id":"probability-puzzles-1","metadata":{"permalink":"/knowledge-base/probability-puzzles-1","source":"@site/blog/2025-07-15-probability-puzzles.md","title":"Probability Puzzles 1","description":"Below are some probability puzzles.","date":"2025-07-15T00:00:00.000Z","tags":[{"inline":false,"label":"Quant","permalink":"/knowledge-base/tags/quant","description":"Quantitative Finance"},{"inline":false,"label":"Stochastic","permalink":"/knowledge-base/tags/stochastic","description":"Stochastic Calculus"},{"inline":false,"label":"Probability","permalink":"/knowledge-base/tags/probability","description":"Probability Theory"},{"inline":false,"label":"Combinatorics","permalink":"/knowledge-base/tags/combinatorics","description":"Combinatorial Mathematics"}],"readingTime":2.055,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"probability-puzzles-1","title":"Probability Puzzles 1","authors":["hinnytsang"],"tags":["quant","stochastic","probability","combinatorics"]},"unlisted":false,"prevItem":{"title":"Finance Knowledge Notes","permalink":"/knowledge-base/finance-knowledges"},"nextItem":{"title":"Paper Review - A Census of the Factor Zoo","permalink":"/knowledge-base/a-census-of-the-factor-zoo"}},"content":"Below are some probability puzzles.\\n\\n\x3c!-- truncate --\x3e\\n\\n1. There are 6 players, numbered 1 to 6. Player 1 rolls a die. If Player 1 rolls a 1, he wins and the game ends.\\n\\n    If Player 1 rolls any other number, the die is passed to the player corresponding to the rolled number, and the game continues with that player rolling. The game ends when a player rolls their own number.\\n\\n    <details>\\n\\n    <summary>Solution</summary>\\n\\n      ```math\\n      p = \\\\frac{1}{6} + \\\\frac{5}{6} \\\\frac{1 - p}{5}\\n      ```\\n\\n    </details>\\n\\n\\n\\n2. A line of 100 airline passengers is waiting to board a plane. They each hold a ticket to one of the 100 seats on that flight. For convenience, let\'s say that the n th passenger in line has a ticket for seat number \'n\'. Being drunk, the first person in line picks a random seat (equally likely for each seat). All of the other passengers are sober, and will go to their assigned seats unless it is already occupied; If it is occupied, they will then find a free seat to sit in, at random. What is the probability that the last (100th) person to board the plane will sit in their own seat (#100)?\\n\\n    <details>\\n\\n    <summary>Solution</summary>\\n\\n      ```math\\n      p = 1 / 100 + 99 / 100 * (1 / 99 + 98 / 99 * (1 / 98 + \\\\cdots ))\\n      p = 1 / 100 + 1 / 100 + 98 / 100 * (1 / 98 + 97 / )\\n      p = 1 / 100 + 1 / 100 + \\\\cdots + 1 / 100\\n      p = 1 / 100 + 1 / 100 + \\\\cdots + 1 / 100\\n      p = 1 / 2\\n      ```\\n\\n    </details>\\n\\n\\n3. Expected number of flip to get 2 heads in a row.\\n\\n    <details>\\n\\n    <summary>Solution</summary>\\n\\n    The answer is simple, let say the expected number is $x$.\\n\\n    1. If we first flip the first tail, we need $x + 1$ flips to get two heads, with prob = $\\\\frac{1}{2}$.\\n\\n    2. If we first flip the first head ($p=\\\\frac{1}{2}$):\\n\\n       1. $1/2$ probability to get a head. Than the total number of flips is $2$.\\n\\n       2. $1/2$ probability to get a tail. Than the total number of flips is $x + 2$ as we already wasted two flips.\\n\\n    ```math\\n    x = \\\\frac{1}{2} (x + 1) + \\\\frac{1}{2} \\\\left( \\\\frac{1}{2} \\\\cdot 2 + \\\\frac{1}{2} (x + 2) \\\\right)\\n    ```\\n\\n    Hance, $x = 6$.\\n\\n    </details>\\n\\n\\n### References\\n\\n\\n1. [CodeChef - Mathematical Expectation](https://www.codechef.com/wiki/tutorial-expectation)"},{"id":"a-census-of-the-factor-zoo","metadata":{"permalink":"/knowledge-base/a-census-of-the-factor-zoo","source":"@site/blog/2025-05-30-a-census-of-the-factor-zoo.md","title":"Paper Review - A Census of the Factor Zoo","description":"Just see a post oh threads suggesting this paper, and I found it very interesting. The author claim that almost all of the past research fails tickle the multiple testing problem, probably appear \'significant\' by chance.","date":"2025-05-30T00:00:00.000Z","tags":[{"inline":false,"label":"Data Mining","permalink":"/knowledge-base/tags/data-mining","description":"Data Mining Techniques"},{"inline":false,"label":"Machine Learning","permalink":"/knowledge-base/tags/machine-learning","description":"Machine Learning"},{"inline":false,"label":"Paper Review","permalink":"/knowledge-base/tags/paper-review","description":"Reviews of Academic Papers"}],"readingTime":0.81,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"a-census-of-the-factor-zoo","title":"Paper Review - A Census of the Factor Zoo","authors":["hinnytsang"],"tags":["data-mining","machine-learning","paper-review"]},"unlisted":false,"prevItem":{"title":"Probability Puzzles 1","permalink":"/knowledge-base/probability-puzzles-1"},"nextItem":{"title":"Generating Function","permalink":"/knowledge-base/generating-function"}},"content":"Just see a post oh threads suggesting this paper, and I found it very interesting. The author claim that almost all of the past research fails tickle the multiple testing problem, probably appear \'significant\' by chance.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe author listed 382 factors published in top journals, and point out that\\n\\n- Papers with positive results tend to be cited more.\\n- Journal with high quality (aka impact factor) usually needs positive results to be published.\\n\\nKey points of the paper:\\n\\n1. More factors involved, more likely to be chance.\\n  - This is known as multiple comparisons problem.\\n  > *The more you look, the more likely you are to find something that looks like a signal, even if it is not a signal.*\\n2. File drawer effect:\\n  - Researcher skip papers they are not excited about (negative results).\\n3. Review the target goal of acceptance rate, but the complexity is need to be considered.\\n4. Academic publication sometimes ignore transaction costs."},{"id":"generating-function","metadata":{"permalink":"/knowledge-base/generating-function","source":"@site/blog/2025-05-30-generating-function.md","title":"Generating Function","description":"I just got asked a question that could be solved by generating function, while I was not familiar with it. So I did some research and found out that generating function is a powerful tool in combinatorics and probability theory. Below are some notes that I jotted after reading openmathbooks.org.","date":"2025-05-30T00:00:00.000Z","tags":[{"inline":false,"label":"Combinatorics","permalink":"/knowledge-base/tags/combinatorics","description":"Combinatorial Mathematics"},{"inline":false,"label":"Probability","permalink":"/knowledge-base/tags/probability","description":"Probability Theory"}],"readingTime":2.775,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"generating-function","title":"Generating Function","authors":["hinnytsang"],"tags":["combinatorics","probability"]},"unlisted":false,"prevItem":{"title":"Paper Review - A Census of the Factor Zoo","permalink":"/knowledge-base/a-census-of-the-factor-zoo"},"nextItem":{"title":"Quant Interview Questions 1","permalink":"/knowledge-base/quant-interview-questions-1"}},"content":"I just got asked a question that could be solved by generating function, while I was not familiar with it. So I did some research and found out that generating function is a powerful tool in combinatorics and probability theory. Below are some notes that I jotted after reading [openmathbooks.org](https://discrete.openmathbooks.org/dmoi2/section-27.html).\\n\\n\x3c!-- truncate --\x3e\\n\\n\\nA generating function is a formal power series whose coefficients correspond to a sequence of numbers. It is often used to encode sequences and to manipulate them algebraically. The idea is to represent a infinite sequence with a single function. Let say we have a sequence,\\n\\n```math\\n1, 2, 3, 4, \\\\ldots\\n```\\n\\nWe can represent it with a infinite series:\\n\\n```math\\nf(x) = 1 + 2x + 3x^2 + 4x^3 + \\\\ldots,\\n```\\n\\nwhere $|x| \\\\leq 1 $ is a variable, this called the generating series of the sequence. If the series converges, i,e,\\n\\n```math\\ng(x) = 1 + 1 + 1 + 1 + \\\\ldots = \\\\frac{1}{1-x},\\n```\\n\\nsuch a function is called a generating function. The coefficients of the series are the terms of the sequence, and the variable $x$ encodes the position of the term in the sequence.\\n\\n## Examples\\n\\nWe can make use of the know generating functions to find the generating function of other sequences. Below are some examples:\\n\\nSwitch $x \\\\to -x$ in the series, we get:\\n\\n```math\\n\\\\frac{1}{1 + x} = 1 - x + x^2 - x^3 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 1, -1, 1, -1, \\\\ldots.\\n```\\n\\nIf $x \\\\to 3x$,\\n```math\\n\\\\frac{1}{1 - 3x} = 1 + 3x + 9x^2 + 27x^3 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 1, 3, 9, 27, \\\\ldots.\\n```\\n\\nWhat about a sequence like $2, 4, 10, 28, \\\\ldots$? It is just like\\n```math\\n(1 + 0) + (1 + 3)x + (1 + 9)x^2 + (1 + 27)x^3 + \\\\ldots = \\\\frac{1}{1 - 3x} + \\\\frac{1}{1 - x}.\\n```\\n\\nIf we replace $x$ with $x^2$ in the series of odd number, we get:\\n\\n```math\\n\\\\frac{1}{1 - x^2} = 1 + x^2 + x^4 + x^6 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 1, 0, 1, 0, 1, \\\\ldots.\\n```\\n\\nSimilarly,\\n\\n```math\\n\\\\frac{1}{1 - x} - \\\\frac{1}{1 - x^2} = x + x^3 + x^5 + x^7 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 0, 1, 0, 1, 0, \\\\ldots.\\n```\\n\\nWe can also take the derivative of the generating function,\\n\\n```math\\n\\\\frac{d}{dx} \\\\left( \\\\frac{1}{1 - x} \\\\right) = \\\\frac{1}{(1 - x)^2} = 1 + 2x + 3x^2 + 4x^3 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 1, 2, 3, 4, \\\\ldots.\\n```\\n\\nWhat if we are interest as the sequence of the odd number? Assume its generating function as $A$\\n\\n```math\\n\\\\begin{align*}\\nA &= 1 + 3x + 5x^2 + 7x^3 + \\\\ldots \\\\\\\\\\nxA &= 1x + 3x^2 + 5x^3 + \\\\ldots \\\\\\\\\\n(1 - x)A &= 1 + 2x  + 2x^2 + 2x^3 + \\\\ldots \\\\\\\\\\n&= 1 + \\\\frac{2x}{1 - x} \\\\\\\\\\n\\\\implies\\nA &= \\\\frac{1}{1 - x} + \\\\frac{2x}{(1 - x) ^ 2} \\\\\\\\\\n\\\\end{align*}\\n```\\n\\n## Recurrence Relations\\n\\nGenerating functions can also be used to solve recurrence relations. For example, consider the Fibonacci sequence:\\n\\n```math\\nF_i = F_{i-1} + F_{i-2}, \\\\quad F_0 = 1, \\\\quad F_1 = 1.\\n```\\nThe generating function for the Fibonacci sequence can be derived as follows:\\n\\nTBC."},{"id":"quant-interview-questions-1","metadata":{"permalink":"/knowledge-base/quant-interview-questions-1","source":"@site/blog/2025-05-30-quant-questions.md","title":"Quant Interview Questions 1","description":"Below are some quant interview questions.","date":"2025-05-30T00:00:00.000Z","tags":[{"inline":false,"label":"Quant","permalink":"/knowledge-base/tags/quant","description":"Quantitative Finance"},{"inline":false,"label":"Stochastic","permalink":"/knowledge-base/tags/stochastic","description":"Stochastic Calculus"}],"readingTime":3.79,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"quant-interview-questions-1","title":"Quant Interview Questions 1","authors":["hinnytsang"],"tags":["quant","stochastic"]},"unlisted":false,"prevItem":{"title":"Generating Function","permalink":"/knowledge-base/generating-function"},"nextItem":{"title":"Generative AI Evaluation Methods","permalink":"/knowledge-base/generative-ai-evaluation-methods"}},"content":"Below are some quant interview questions.\\n\\n\x3c!-- truncate --\x3e\\n\\n1. Difference between European, American and Bermudan options.\\n\\n    <details>\\n    <summary>Answer</summary>\\n\\n    - European Options: Can only be exercised at expiration.\\n    - American Options: Can be exercised at any time before expiration.\\n    - Bermudan Options: Can be exercised on specific dates before expiration.\\n\\n    </details>\\n\\n2. What are the assumptions of Black-Scholes model?\\n\\n    <details>\\n    <summary>Answer</summary>\\n    - European options.\\n    - Geometric Brownian motion.\\n    - No arbitrage opportunities.\\n    - Constant volatility.\\n    - Constant interest rate.\\n    - No dividends.\\n    - Frictionless Markets.\\n    - Perfectly hedged.\\n    - Continuous trading.\\n    - Infinite liquidity.\\n    </details>\\n\\n\\n3. Derive the Black-Scholes formula.\\n\\n    <details>\\n    <summary>Answer</summary>\\n\\n    Geometric Brownian motion of the stock price $`S_t`$:\\n\\n    ```math\\n    dS_t = \\\\mu S_t dt + \\\\sigma S_t dW_t\\n    ```\\n\\n    where $`\\\\mu`$ is the drift, $`\\\\sigma`$ is the volatility, and $`W_t`$ is a Wiener process. It states that the infinitesimal rate of return on the stock has an expected value of $`\\\\mu dt`$ and a variance of $`\\\\sigma^2 dt`$.\\n\\n    The value of the options depends on stock price $`S_t`$, time $`t`$, denoted as $`V(S_t, t)`$. Hence, ito\'s lemma gives:\\n\\n    ```math\\n    \\\\begin{align*}\\n    d V(S_t, t)\\n    &= \\\\left(\\n      \\\\frac{\\\\partial V}{\\\\partial t}\\n      + \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt\\n    + \\\\frac{\\\\partial V}{\\\\partial S} d S \\\\\\\\\\n    &= \\\\left(\\n      \\\\frac{\\\\partial V}{\\\\partial t}\\n      + \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt\\n    + \\\\frac{\\\\partial V}{\\\\partial S}\\\\left(\\n      \\\\mu S_t dt + \\\\sigma S_t dW_t\\n    \\\\right) \\\\\\\\\\n    &= \\\\left(\\n      \\\\frac{\\\\partial V}{\\\\partial t}\\n      + \\\\mu S \\\\frac{\\\\partial V}{\\\\partial S}\\n      + \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt\\n    + \\\\sigma S \\\\frac{\\\\partial V}{\\\\partial S}dW_t\\\\\\\\\\n    \\\\end{align*}\\n    ```\\n\\n    Base on the behavior of the option price, we construct a portfolio that consists of the option and a short position in $`S_t`$:\\n\\n    ```math\\n    \\\\Pi = - V(S_t, t) + \\\\frac{\\\\partial V}{\\\\partial S} S_t\\n    ```\\n    > It is a Delta ($\\\\Delta = \\\\frac{\\\\partial V}{\\\\partial S}$) hedging using the stock. Thus the net delta is zero.\\n\\n    And assuming the portfolio is self-financing (no additional cash flow), we have (Assuming delta is constant):\\n\\n    ```math\\n    d\\\\Pi = - dV + \\\\frac{\\\\partial V}{\\\\partial S} dS_t\\n    ```\\n\\n    By substituting the expression of $`dV`$ and $`dS_t`$ into the equation, we have:\\n\\n    ```math\\n    d\\\\Pi = \\\\left(\\n      - \\\\frac{\\\\partial V}{\\\\partial t}\\n      - \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt\\n    ```\\n\\n    Noted that the stochastic term is eliminated. Let\'s assume the portfolio have a risk-free return $`r`$, i.e.\\n\\n    ```math\\n    d\\\\Pi = r \\\\Pi dt\\n    ```\\n\\n    Thus,\\n\\n    ```math\\n    \\\\left(\\n      - \\\\frac{\\\\partial V}{\\\\partial t}\\n      - \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt = r \\\\left( - V + \\\\frac{\\\\partial V}{\\\\partial S} S_t \\\\right) dt\\n    ```\\n\\n    Rearranging the equation gives:\\n\\n    ```math\\n    \\\\frac{\\\\partial V}{\\\\partial t} + r S_t \\\\frac{\\\\partial V}{\\\\partial S}\\n    + \\\\frac{1}{2} \\\\sigma^2 S_t^2 \\\\frac{\\\\partial^2 V}{\\\\partial S^2} - r V = 0\\n    ```\\n\\n    This is the Black-Scholes PDE.\\n\\n    </details>\\n\\n\\n4. Sample 3 from [0, 1], what is the expectation of the maximum?\\n\\n    <details>\\n    <summary>Answer</summary>\\n\\n    Let say $`X_1, X_2, X_3`$ are the three samples from uniform distribution $`U(0, 1)`$. The maximum is $`M = \\\\max(X_1, X_2, X_3)`$. The cumulative distribution function (CDF) of $`M`$ is:\\n\\n    ```math\\n    P(M \\\\leq x) = P(X_1 \\\\leq x, X_2 \\\\leq x, X_3 \\\\leq x) = P(X_1 \\\\leq x) P(X_2 \\\\leq x) P(X_3 \\\\leq x) = x^3\\n    ```\\n    The probability density function (PDF) is:\\n\\n    ```math\\n    f_M(x) = \\\\frac{d}{dx} P(M \\\\leq x) = 3x^2\\n    ```\\n    The expectation of $`M`$ is:\\n\\n    ```math\\n    E[M] = \\\\int_0^1 x f_M(x) dx = \\\\int_0^1 x \\\\cdot 3x^2 dx = 3 \\\\int_0^1 x^3 dx = 3 \\\\cdot \\\\frac{1}{4} = \\\\frac{3}{4}\\n    ```\\n\\n    </details>\\n\\n\\n5.  We have a stick with unit length which we cut randomly at two spots. What is the expected length of the tallest piece?\\n\\n    <details>\\n    <summary>Answer</summary>\\n\\n    Let $`X_1`$ and $`X_2`$ be the two random cut points, uniformly distributed over the interval $[0, 1]$. Without loss of generality, assume $`X_1 \\\\leq X_2`$. The lengths of the three pieces are:\\n\\n    - Left piece: $`X_1`$\\n    - Middle piece: $`X_2 - X_1`$\\n    - Right piece: $`1 - X_2`$\\n\\n    The maximum length is:\\n\\n    ```math\\n    M = \\\\max(X_1, X_2 - X_1, 1 - X_2)\\n    ```\\n\\n    To find the expected value of $`M`$, we can use the law of total expectation. The joint distribution of $`X_1`$ and $`X_2`$ is uniform over the unit square. We can compute the expected value by integrating over the region where $`X_1 \\\\leq X_2`$.\\n\\n    The expected value can be computed as:\\n\\n    ```math\\n    E[M] = \\\\int_0^1 \\\\int_{x_1}^1 \\\\max(x_1, x_2 - x_1, 1 - x_2) dx_2 dx_1\\n    ```\\n\\n    After evaluating this integral, we find that:\\n\\n    ```math\\n    E[M] = \\\\frac{5}{6}\\n    ```\\n\\n    </details>\\n\\n\\n6. Moving dot and polygon relative position detection. Calculate the time complexity"},{"id":"generative-ai-evaluation-methods","metadata":{"permalink":"/knowledge-base/generative-ai-evaluation-methods","source":"@site/blog/2025-05-16-generative-ai-evaluation.md","title":"Generative AI Evaluation Methods","description":"The evaluation of generative AI models is a complex task that requires a combination of qualitative and quantitative methods. Here is the points that I summarized from the Machine Learning Operations with Vertex AI tutorial from Google.","date":"2025-05-16T00:00:00.000Z","tags":[{"inline":false,"label":"Generative AI","permalink":"/knowledge-base/tags/generative-ai","description":"Generative AI"}],"readingTime":2.145,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"generative-ai-evaluation-methods","title":"Generative AI Evaluation Methods","authors":["hinnytsang"],"tags":["generative-ai"]},"unlisted":false,"prevItem":{"title":"Quant Interview Questions 1","permalink":"/knowledge-base/quant-interview-questions-1"},"nextItem":{"title":"Ito\'s Lamma","permalink":"/knowledge-base/ito-lamma"}},"content":"The evaluation of generative AI models is a complex task that requires a combination of qualitative and quantitative methods. Here is the points that I summarized from the Machine Learning Operations with Vertex AI tutorial from Google.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn general, the evaluation methods can be categorized into several types:\\n\\n1. Binary Evaluation\\n    - e.g. positive or negative sentiment analysis, spam detection, and appropriate content detection.\\n\\n2. Category Evaluation\\n    - e.g. topic classification, sentiment analysis including neutral, and product rating.\\n\\n3. Ranking Evaluation\\n    - Rank the relative quality for different outputs.\\n    - e.g. ranking search results, ranking product recommendations, and ranking news articles.\\n\\n4. Numerical Evaluation\\n    - Assigns a quantitative score to model output.\\n    - e.g. BLEU, ROUGE, METEOR, perplexity, and F1 score.\\n\\n5. Text Evaluation\\n    - Evaluation of the text by human.\\n\\n6. Multi-task Evaluation\\n    - Mixture of the above methods.\\n\\nThe choice of evaluation method depends on the specific task and the goals of the evaluation. Below are some examples:\\n\\n1. Lexical Similarity\\n    - Measure the similarity between the model\'s output and reference text, based on word overlap, sequence of words, or semantic similarity.\\n    - e.g. BLEU focuses on n-gram overlap, ROUGE focuses on recall, and METEOR focuses on both precision and recall.\\n\\n2. Linguistic Quality\\n    - Evaluate the fluency, coherence, and grammatical correctness of the generated text.\\n    - e.g. Perplexity, BLEURT.\\n\\n3. Task-Specific Evaluation\\n    - Evaluate the performance of the model on specific tasks, such as summarization, translation, or question answering.\\n    - e.g. BLEU for translation, and ROUGE for summarization.\\n\\n4. Safety and fairness\\n    - Evaluate the model\'s output for safety and fairness, including bias detection and harmful content detection.\\n    - e.g. toxicity detection, hate speech detection, and bias detection or even human evaluation.\\n\\n5. Groundedness\\n    - Evaluate the factual accuracy of the model\'s output.\\n    - e.g. Fact-checking tools, knowledge-based integration and human evaluation.\\n\\n6. User-Centric Evaluation\\n    - Focus on the user experience and satisfaction with the model\'s output.\\n    - e.g. User survey.\\n\\nMoreover, there are some common evaluation paradigms:\\n\\n1. Pointwise Evaluation:\\n    - Evaluating model behavior in production.\\n    - Absolute performance of a single model.\\n    - Identifying behaviors to prioritize for tuning.\\n    - Establishing a baseline for model performance.\\n\\n2. Pairwise Evaluation:\\n    - Comparison of two models.\\n\\nEvaluation methods are not only computation based, but also be model based, using LLM as a judge to evaluate the model output (Google Auto Side by Side). In summary, the choice of evaluation method depends on the specific task and the goals of the evaluation. I may talk about more about the evaluation metrics in the future."},{"id":"ito-lamma","metadata":{"permalink":"/knowledge-base/ito-lamma","source":"@site/blog/2025-05-13-ito-lamma.md","title":"Ito\'s Lamma","description":"In very simple terms, Ito\'s lemma is a formula that allows us to compute the differential of a function of a stochastic process. In normal functions, we can use the chain rule to compute the differential of a function. However, in stochastic calculus, we need to use Ito\'s lemma.","date":"2025-05-13T00:00:00.000Z","tags":[{"inline":false,"label":"Stochastic","permalink":"/knowledge-base/tags/stochastic","description":"Stochastic Calculus"}],"readingTime":1.34,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"ito-lamma","title":"Ito\'s Lamma","authors":["hinnytsang"],"tags":["stochastic"]},"unlisted":false,"prevItem":{"title":"Generative AI Evaluation Methods","permalink":"/knowledge-base/generative-ai-evaluation-methods"},"nextItem":{"title":"Machine Learning Techniques","permalink":"/knowledge-base/ml-techniques"}},"content":"In very simple terms, Ito\'s lemma is a formula that allows us to compute the differential of a function of a stochastic process. In normal functions, we can use the chain rule to compute the differential of a function. However, in stochastic calculus, we need to use Ito\'s lemma.\\n\\n\x3c!-- truncate --\x3e\\n\\nSuppose we have a stochastic process $X_t$ that follows a stochastic differential equation (SDE).\\n\\n$$\\ndX_t = \\\\mu(X_t, t) dt + \\\\sigma(X_t, t) dW_t\\n$$\\n\\nwhere $W_t$ is a Wiener process (or Brownian motion), $\\\\mu(X_t, t)$ is the drift term, and $\\\\sigma(X_t, t)$ is the diffusion term.\\n\\nNow, let\'s say we have a function $f(X_t, t)$ (twice differentiable) that we want to differentiate with respect to time. Ito\'s lemma states that the differential of $f(X_t, t)$ is given by:\\n\\n$$\\n\\\\frac{\\\\Delta f(t)}{dt} dt\\n= \\\\frac{\\\\partial f}{\\\\partial t} dt\\n+ \\\\frac{1}{2}\\\\frac{\\\\partial^2f}{\\\\partial t^2} (dt)^2\\n+ \\\\cdots\\n$$\\n\\nand so with $x$, the total derivative of $f$ will be\\n\\n```math\\n\\\\begin{align*}\\ndf &= f_t dt + f_x dx \\\\\\\\\\n&=\\\\lim_{dx, dt\\\\rightarrow 0, 0} \\\\frac{\\\\partial f}{\\\\partial t} dt\\n+ \\\\frac{\\\\partial f}{\\\\partial x} dx\\n+ \\\\frac{1}{2} \\\\left(\\\\frac{\\\\partial^2f}{\\\\partial t^2} (dt)^2\\n+ \\\\frac{\\\\partial^2f}{\\\\partial x^2} (dx)^2 \\\\right)\\n+ \\\\cdots\\n\\\\end{align*}\\n```\\n\\nThan substitute $x = X_t$, in the limit $dt \\\\to 0$, the following terms tend to zero faster than $dt$ are:\\n\\n1. $(dt)^2$\\n\\n2. $dtdW_t$\\n\\n3. $(dx)^3$\\n\\nNoted that $(dB_t)^2 = \\\\mathcal{O}(dt)$ due to quadratic variation of a Wiener process. # TODO,\\nHance\\n\\n$$\\ndf\\n= \\\\lim_{dt \\\\to 0} \\\\left(\\\\frac{\\\\partial f}{\\\\partial t}\\n+ \\\\mu_t \\\\frac{\\\\partial f}{\\\\partial x}\\n+ \\\\frac{\\\\sigma_t^2}{2} \\\\frac{\\\\partial^2 f}{\\\\partial x^2} \\\\right) dt\\n+ \\\\sigma_t \\\\frac{\\\\partial f}{\\\\partial x} dW_t\\n$$\\n\\nThat\'s it!"},{"id":"ml-techniques","metadata":{"permalink":"/knowledge-base/ml-techniques","source":"@site/blog/2025-05-13-ml-techniques.md","title":"Machine Learning Techniques","description":"Notes to MLE interviews.","date":"2025-05-13T00:00:00.000Z","tags":[{"inline":false,"label":"Machine Learning","permalink":"/knowledge-base/tags/machine-learning","description":"Machine Learning"}],"readingTime":3.075,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"ml-techniques","title":"Machine Learning Techniques","authors":["hinnytsang"],"tags":["machine-learning"]},"unlisted":false,"prevItem":{"title":"Ito\'s Lamma","permalink":"/knowledge-base/ito-lamma"}},"content":"Notes to MLE interviews.\\n\\n\x3c!-- truncate --\x3e\\n\\n### One-Hot Encoding\\n\\nDoesn\'t work well with high cardinality categorical features, and tree-based models like XGBoost and LightGBM due to too many zeros.\\n\\n### Mean Encoding\\n\\nlet say sample is below.\\n\\n| Age | Income |\\n| --- | ------ |\\n| 18  | 60,000 |\\n| 18  | 50,000 |\\n| 18  | 40,000 |\\n| 19  | 66,000 |\\n| 19  | 51,000 |\\n| 19  | 42,000 |\\n\\nMean encoding encode age by the mean of income, i.e.\\n\\n| Age | Income | Mean Encoding |\\n| --- | ------ | ------------- |\\n| 18  | 60,000 | 50,000        |\\n| 18  | 50,000 | 50,000        |\\n| 18  | 40,000 | 50,000        |\\n| 19  | 66,000 | 53,000        |\\n| 19  | 51,000 | 53,000        |\\n| 19  | 42,000 | 53,000        |\\n\\nBe aware of label leakage. Can apply additive smoothing to make it more robust.\\n\\n### Feature Hashing\\n\\nMap to a fixed number of features. One problem is collision if hash size is too small.\\n\\n### Cross Feature\\n\\nJoin categorical features together. For example, if we have two categorical features, `A` and `B`, we can create a new feature `C` that is the concatenation of `A` and `B`. This can help capture interactions between the two features.\\n\\n### Embedding\\n\\nA way to represent categorical features as continuous vectors. This is often used in deep learning models, where we can learn the embeddings during training. Embeddings can capture complex relationships between categories and are particularly useful for high cardinality features.\\n\\n- Continuous Bag of Words (CBOW): words[t-n] to word[t-1], word[t+1] to word[t+n] to predict word[t]\\n- Skip-gram: word[t] to predict surrounding words.\\n\\n:::note\\n\\nRule of thumb: $ d = D4d = D^{1/4} $ where $D$ is the number of categories and $d$ is the dimension of the embedding.\\n\\n:::\\n\\n:::tip\\n\\nPre-trained word embeddings (ex. word2vec, word2glove, etc.) are powerful.\\n\\n:::\\n\\n- Word to vector..\\n\\n### Attention Mechanism\\n\\nGiven embedding $E_i$ for each word $w_i$,\\n\\n1. Query $Q_i = W_q E_i$ for specfic query.\\n\\n2. Key $K_i = W_k E_i$\\n\\n3. Check similarity between query and key, $S_{ij} = Q_i \\\\cdot K_j$. High similiarity means $K_i$ *attends to* $Q_j$!\\n\\n4. Normalize $S_{ij}$ by softmax to $A_i = \\\\text{softmax}(S_{i}) V$ into attention pattern.\\n    - Masking: Set all $S_{ij}$ to $-\\\\infty$ if $j < i$ to prevent attending to future words.\\n\\n5. Value $V_i = W_v E_j$ the changes to the embedding from prevous embedding.\\n\\n6. Update embedding buy sum of previous values and weighted by attention pattern, $E_i\' = \\\\sum_j A_{ij} V_j$.\\n\\n- Cross attention: $Q_i$ is from one sequence, $K_j$ and $V_j$ are from another sequence, e.g. for translation.\\n\\n- Multi-head attention: Use multiple sets of $W_q$, $W_k$, and $W_v$ to capture different aspects of the input. Then concatenate the outputs from each head.\\n   - Pros: Allows the model to focus on different parts of the input simultaneously (Parallel processing).\\n\\n\\n More layers: e.g. Attention -> Multi-layer perceptron (MLP) -> Attention -> ... in order to capture more complex relationships.\\n\\n\\n### Transformer\\n\\nSituations, predict next token given previous tokens.\\n\\nStep structures:\\n\\n1. Input embedding: Convert input tokens into continuous vectors.\\n\\n2. Positional encoding: Add positional information to the input embeddings to capture the order of tokens.\\n\\n3. Multi-head self-attention: Compute attention scores for each token with respect to all other tokens in the sequence.\\n\\n4. Feed-forward neural network: Apply a feed-forward network to each token independently.\\n\\n5. Layer normalization: Normalize the output of the feed-forward network.\\n\\n6. Residual connection: Add the input of the layer to the output to help with gradient flow.\\n\\n7. Output layer: Convert the final representations back to token probabilities."}]}}')}}]);