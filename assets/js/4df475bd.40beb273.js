"use strict";(self.webpackChunkknowledge_base=self.webpackChunkknowledge_base||[]).push([[739],{9509:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"a-census-of-the-factor-zoo","metadata":{"permalink":"/knowledge-base/a-census-of-the-factor-zoo","source":"@site/blog/2025-05-30-a-census-of-the-factor-zoo.md","title":"Paper Review - A Census of the Factor Zoo","description":"Just see a post oh threads suggesting this paper, and I found it very interesting. The author claim that almost all of the past research fails tickle the multiple testing problem, probably appear \'significant\' by chance.","date":"2025-05-30T00:00:00.000Z","tags":[{"inline":false,"label":"Data Mining","permalink":"/knowledge-base/tags/data-mining","description":"Data Mining Techniques"},{"inline":false,"label":"Machine Learning","permalink":"/knowledge-base/tags/machine-learning","description":"Machine Learning"},{"inline":false,"label":"Paper Review","permalink":"/knowledge-base/tags/paper-review","description":"Reviews of Academic Papers"}],"readingTime":0.81,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"a-census-of-the-factor-zoo","title":"Paper Review - A Census of the Factor Zoo","authors":["hinnytsang"],"tags":["data-mining","machine-learning","paper-review"]},"unlisted":false,"nextItem":{"title":"Generating Function","permalink":"/knowledge-base/generating-function"}},"content":"Just see a post oh threads suggesting this paper, and I found it very interesting. The author claim that almost all of the past research fails tickle the multiple testing problem, probably appear \'significant\' by chance.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe author listed 382 factors published in top journals, and point out that\\n\\n- Papers with positive results tend to be cited more.\\n- Journal with high quality (aka impact factor) usually needs positive results to be published.\\n\\nKey points of the paper:\\n\\n1. More factors involved, more likely to be chance.\\n  - This is known as multiple comparisons problem.\\n  > *The more you look, the more likely you are to find something that looks like a signal, even if it is not a signal.*\\n2. File drawer effect:\\n  - Researcher skip papers they are not excited about (negative results).\\n3. Review the target goal of acceptance rate, but the complexity is need to be considered.\\n4. Academic publication sometimes ignore transaction costs."},{"id":"generating-function","metadata":{"permalink":"/knowledge-base/generating-function","source":"@site/blog/2025-05-30-generating-function.md","title":"Generating Function","description":"I just got asked a question that could be solved by generating function, while I was not familiar with it. So I did some research and found out that generating function is a powerful tool in combinatorics and probability theory. Below are some notes that I jotted after reading openmathbooks.org.","date":"2025-05-30T00:00:00.000Z","tags":[{"inline":false,"label":"Combinatorics","permalink":"/knowledge-base/tags/combinatorics","description":"Combinatorial Mathematics"},{"inline":false,"label":"Probability","permalink":"/knowledge-base/tags/probability","description":"Probability Theory"}],"readingTime":2.775,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"generating-function","title":"Generating Function","authors":["hinnytsang"],"tags":["combinatorics","probability"]},"unlisted":false,"prevItem":{"title":"Paper Review - A Census of the Factor Zoo","permalink":"/knowledge-base/a-census-of-the-factor-zoo"},"nextItem":{"title":"Quant Interview Questions 1","permalink":"/knowledge-base/quant-interview-questions-1"}},"content":"I just got asked a question that could be solved by generating function, while I was not familiar with it. So I did some research and found out that generating function is a powerful tool in combinatorics and probability theory. Below are some notes that I jotted after reading [openmathbooks.org](https://discrete.openmathbooks.org/dmoi2/section-27.html).\\n\\n\x3c!-- truncate --\x3e\\n\\n\\nA generating function is a formal power series whose coefficients correspond to a sequence of numbers. It is often used to encode sequences and to manipulate them algebraically. The idea is to represent a infinite sequence with a single function. Let say we have a sequence,\\n\\n```math\\n1, 2, 3, 4, \\\\ldots\\n```\\n\\nWe can represent it with a infinite series:\\n\\n```math\\nf(x) = 1 + 2x + 3x^2 + 4x^3 + \\\\ldots,\\n```\\n\\nwhere $|x| \\\\leq 1 $ is a variable, this called the generating series of the sequence. If the series converges, i,e,\\n\\n```math\\ng(x) = 1 + 1 + 1 + 1 + \\\\ldots = \\\\frac{1}{1-x},\\n```\\n\\nsuch a function is called a generating function. The coefficients of the series are the terms of the sequence, and the variable $x$ encodes the position of the term in the sequence.\\n\\n## Examples\\n\\nWe can make use of the know generating functions to find the generating function of other sequences. Below are some examples:\\n\\nSwitch $x \\\\to -x$ in the series, we get:\\n\\n```math\\n\\\\frac{1}{1 + x} = 1 - x + x^2 - x^3 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 1, -1, 1, -1, \\\\ldots.\\n```\\n\\nIf $x \\\\to 3x$,\\n```math\\n\\\\frac{1}{1 - 3x} = 1 + 3x + 9x^2 + 27x^3 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 1, 3, 9, 27, \\\\ldots.\\n```\\n\\nWhat about a sequence like $2, 4, 10, 28, \\\\ldots$? It is just like\\n```math\\n(1 + 0) + (1 + 3)x + (1 + 9)x^2 + (1 + 27)x^3 + \\\\ldots = \\\\frac{1}{1 - 3x} + \\\\frac{1}{1 - x}.\\n```\\n\\nIf we replace $x$ with $x^2$ in the series of odd number, we get:\\n\\n```math\\n\\\\frac{1}{1 - x^2} = 1 + x^2 + x^4 + x^6 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 1, 0, 1, 0, 1, \\\\ldots.\\n```\\n\\nSimilarly,\\n\\n```math\\n\\\\frac{1}{1 - x} - \\\\frac{1}{1 - x^2} = x + x^3 + x^5 + x^7 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 0, 1, 0, 1, 0, \\\\ldots.\\n```\\n\\nWe can also take the derivative of the generating function,\\n\\n```math\\n\\\\frac{d}{dx} \\\\left( \\\\frac{1}{1 - x} \\\\right) = \\\\frac{1}{(1 - x)^2} = 1 + 2x + 3x^2 + 4x^3 + \\\\ldots \\\\quad \\\\text{which generates} \\\\quad 1, 2, 3, 4, \\\\ldots.\\n```\\n\\nWhat if we are interest as the sequence of the odd number? Assume its generating function as $A$\\n\\n```math\\n\\\\begin{align*}\\nA &= 1 + 3x + 5x^2 + 7x^3 + \\\\ldots \\\\\\\\\\nxA &= 1x + 3x^2 + 5x^3 + \\\\ldots \\\\\\\\\\n(1 - x)A &= 1 + 2x  + 2x^2 + 2x^3 + \\\\ldots \\\\\\\\\\n&= 1 + \\\\frac{2x}{1 - x} \\\\\\\\\\n\\\\implies\\nA &= \\\\frac{1}{1 - x} + \\\\frac{2x}{(1 - x) ^ 2} \\\\\\\\\\n\\\\end{align*}\\n```\\n\\n## Recurrence Relations\\n\\nGenerating functions can also be used to solve recurrence relations. For example, consider the Fibonacci sequence:\\n\\n```math\\nF_i = F_{i-1} + F_{i-2}, \\\\quad F_0 = 1, \\\\quad F_1 = 1.\\n```\\nThe generating function for the Fibonacci sequence can be derived as follows:\\n\\nTBC."},{"id":"quant-interview-questions-1","metadata":{"permalink":"/knowledge-base/quant-interview-questions-1","source":"@site/blog/2025-05-30-quant-questions.md","title":"Quant Interview Questions 1","description":"Below are some quant interview questions.","date":"2025-05-30T00:00:00.000Z","tags":[{"inline":false,"label":"Quant","permalink":"/knowledge-base/tags/quant","description":"Quantitative Finance"},{"inline":false,"label":"Stochastic","permalink":"/knowledge-base/tags/stochastic","description":"Stochastic Calculus"}],"readingTime":2.3,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"quant-interview-questions-1","title":"Quant Interview Questions 1","authors":["hinnytsang"],"tags":["quant","stochastic"]},"unlisted":false,"prevItem":{"title":"Generating Function","permalink":"/knowledge-base/generating-function"},"nextItem":{"title":"Generative AI Evaluation Methods","permalink":"/knowledge-base/generative-ai-evaluation-methods"}},"content":"Below are some quant interview questions.\\n\\n\x3c!-- truncate --\x3e\\n\\n1. Difference between European, American and Bermudan options.\\n\\n    <details>\\n    <summary>Answer</summary>\\n\\n    - European Options: Can only be exercised at expiration.\\n    - American Options: Can be exercised at any time before expiration.\\n    - Bermudan Options: Can be exercised on specific dates before expiration.\\n\\n    </details>\\n\\n2. What are the assumptions of Black-Scholes model?\\n\\n    <details>\\n    <summary>Answer</summary>\\n    - European options.\\n    - Geometric Brownian motion.\\n    - No arbitrage opportunities.\\n    - Constant volatility.\\n    - Constant interest rate.\\n    - No dividends.\\n    - Frictionless Markets.\\n    - Perfectly hedged.\\n    - Continuous trading.\\n    - Infinite liquidity.\\n    </details>\\n\\n\\n3. Derive the Black-Scholes formula.\\n\\n    <details>\\n    <summary>Answer</summary>\\n\\n    Geometric Brownian motion of the stock price $`S_t`$:\\n\\n    ```math\\n    dS_t = \\\\mu S_t dt + \\\\sigma S_t dW_t\\n    ```\\n\\n    where $`\\\\mu`$ is the drift, $`\\\\sigma`$ is the volatility, and $`W_t`$ is a Wiener process. It states that the infinitesimal rate of return on the stock has an expected value of $`\\\\mu dt`$ and a variance of $`\\\\sigma^2 dt`$.\\n\\n    The value of the options depends on stock price $`S_t`$, time $`t`$, denoted as $`V(S_t, t)`$. Hence, ito\'s lemma gives:\\n\\n    ```math\\n    \\\\begin{align*}\\n    d V(S_t, t)\\n    &= \\\\left(\\n      \\\\frac{\\\\partial V}{\\\\partial t}\\n      + \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt\\n    + \\\\frac{\\\\partial V}{\\\\partial S} d S \\\\\\\\\\n    &= \\\\left(\\n      \\\\frac{\\\\partial V}{\\\\partial t}\\n      + \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt\\n    + \\\\frac{\\\\partial V}{\\\\partial S}\\\\left(\\n      \\\\mu S_t dt + \\\\sigma S_t dW_t\\n    \\\\right) \\\\\\\\\\n    &= \\\\left(\\n      \\\\frac{\\\\partial V}{\\\\partial t}\\n      + \\\\mu S \\\\frac{\\\\partial V}{\\\\partial S}\\n      + \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt\\n    + \\\\sigma S \\\\frac{\\\\partial V}{\\\\partial S}dW_t\\\\\\\\\\n    \\\\end{align*}\\n    ```\\n\\n    Base on the behavior of the option price, we construct a portfolio that consists of the option and a short position in $`S_t`$:\\n\\n    ```math\\n    \\\\Pi = - V(S_t, t) + \\\\frac{\\\\partial V}{\\\\partial S} S_t\\n    ```\\n    > It is a Delta ($\\\\Delta = \\\\frac{\\\\partial V}{\\\\partial S}$) hedging using the stock. Thus the net delta is zero.\\n\\n    And assuming the portfolio is self-financing (no additional cash flow), we have (Assuming delta is constant):\\n\\n    ```math\\n    d\\\\Pi = - dV + \\\\frac{\\\\partial V}{\\\\partial S} dS_t\\n    ```\\n\\n    By substituting the expression of $`dV`$ and $`dS_t`$ into the equation, we have:\\n\\n    ```math\\n    d\\\\Pi = \\\\left(\\n      - \\\\frac{\\\\partial V}{\\\\partial t}\\n      - \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt\\n    ```\\n\\n    Noted that the stochastic term is eliminated. Let\'s assume the portfolio have a risk-free return $`r`$, i.e.\\n\\n    ```math\\n    d\\\\Pi = r \\\\Pi dt\\n    ```\\n\\n    Thus,\\n\\n    ```math\\n    \\\\left(\\n      - \\\\frac{\\\\partial V}{\\\\partial t}\\n      - \\\\frac{1}{2} \\\\frac{\\\\partial^2 V}{\\\\partial S^2} \\\\sigma^2 S_t^2\\n    \\\\right) dt = r \\\\left( - V + \\\\frac{\\\\partial V}{\\\\partial S} S_t \\\\right) dt\\n    ```\\n\\n    Rearranging the equation gives:\\n\\n    ```math\\n    \\\\frac{\\\\partial V}{\\\\partial t} + r S_t \\\\frac{\\\\partial V}{\\\\partial S}\\n    + \\\\frac{1}{2} \\\\sigma^2 S_t^2 \\\\frac{\\\\partial^2 V}{\\\\partial S^2} - r V = 0\\n    ```\\n\\n    This is the Black-Scholes PDE.\\n\\n    </details>"},{"id":"generative-ai-evaluation-methods","metadata":{"permalink":"/knowledge-base/generative-ai-evaluation-methods","source":"@site/blog/2025-05-16-generative-ai-evaluation.md","title":"Generative AI Evaluation Methods","description":"The evaluation of generative AI models is a complex task that requires a combination of qualitative and quantitative methods. Here is the points that I summarized from the Machine Learning Operations with Vertex AI tutorial from Google.","date":"2025-05-16T00:00:00.000Z","tags":[{"inline":false,"label":"Generative AI","permalink":"/knowledge-base/tags/generative-ai","description":"Generative AI"}],"readingTime":2.145,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"generative-ai-evaluation-methods","title":"Generative AI Evaluation Methods","authors":["hinnytsang"],"tags":["generative-ai"]},"unlisted":false,"prevItem":{"title":"Quant Interview Questions 1","permalink":"/knowledge-base/quant-interview-questions-1"},"nextItem":{"title":"Ito\'s Lamma","permalink":"/knowledge-base/ito-lamma"}},"content":"The evaluation of generative AI models is a complex task that requires a combination of qualitative and quantitative methods. Here is the points that I summarized from the Machine Learning Operations with Vertex AI tutorial from Google.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn general, the evaluation methods can be categorized into several types:\\n\\n1. Binary Evaluation\\n    - e.g. positive or negative sentiment analysis, spam detection, and appropriate content detection.\\n\\n2. Category Evaluation\\n    - e.g. topic classification, sentiment analysis including neutral, and product rating.\\n\\n3. Ranking Evaluation\\n    - Rank the relative quality for different outputs.\\n    - e.g. ranking search results, ranking product recommendations, and ranking news articles.\\n\\n4. Numerical Evaluation\\n    - Assigns a quantitative score to model output.\\n    - e.g. BLEU, ROUGE, METEOR, perplexity, and F1 score.\\n\\n5. Text Evaluation\\n    - Evaluation of the text by human.\\n\\n6. Multi-task Evaluation\\n    - Mixture of the above methods.\\n\\nThe choice of evaluation method depends on the specific task and the goals of the evaluation. Below are some examples:\\n\\n1. Lexical Similarity\\n    - Measure the similarity between the model\'s output and reference text, based on word overlap, sequence of words, or semantic similarity.\\n    - e.g. BLEU focuses on n-gram overlap, ROUGE focuses on recall, and METEOR focuses on both precision and recall.\\n\\n2. Linguistic Quality\\n    - Evaluate the fluency, coherence, and grammatical correctness of the generated text.\\n    - e.g. Perplexity, BLEURT.\\n\\n3. Task-Specific Evaluation\\n    - Evaluate the performance of the model on specific tasks, such as summarization, translation, or question answering.\\n    - e.g. BLEU for translation, and ROUGE for summarization.\\n\\n4. Safety and fairness\\n    - Evaluate the model\'s output for safety and fairness, including bias detection and harmful content detection.\\n    - e.g. toxicity detection, hate speech detection, and bias detection or even human evaluation.\\n\\n5. Groundedness\\n    - Evaluate the factual accuracy of the model\'s output.\\n    - e.g. Fact-checking tools, knowledge-based integration and human evaluation.\\n\\n6. User-Centric Evaluation\\n    - Focus on the user experience and satisfaction with the model\'s output.\\n    - e.g. User survey.\\n\\nMoreover, there are some common evaluation paradigms:\\n\\n1. Pointwise Evaluation:\\n    - Evaluating model behavior in production.\\n    - Absolute performance of a single model.\\n    - Identifying behaviors to prioritize for tuning.\\n    - Establishing a baseline for model performance.\\n\\n2. Pairwise Evaluation:\\n    - Comparison of two models.\\n\\nEvaluation methods are not only computation based, but also be model based, using LLM as a judge to evaluate the model output (Google Auto Side by Side). In summary, the choice of evaluation method depends on the specific task and the goals of the evaluation. I may talk about more about the evaluation metrics in the future."},{"id":"ito-lamma","metadata":{"permalink":"/knowledge-base/ito-lamma","source":"@site/blog/2025-05-13-ito-lamma.md","title":"Ito\'s Lamma","description":"In very simple terms, Ito\'s lemma is a formula that allows us to compute the differential of a function of a stochastic process. In normal functions, we can use the chain rule to compute the differential of a function. However, in stochastic calculus, we need to use Ito\'s lemma.","date":"2025-05-13T00:00:00.000Z","tags":[{"inline":false,"label":"Stochastic","permalink":"/knowledge-base/tags/stochastic","description":"Stochastic Calculus"}],"readingTime":1.34,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"ito-lamma","title":"Ito\'s Lamma","authors":["hinnytsang"],"tags":["stochastic"]},"unlisted":false,"prevItem":{"title":"Generative AI Evaluation Methods","permalink":"/knowledge-base/generative-ai-evaluation-methods"},"nextItem":{"title":"Machine Learning Techniques","permalink":"/knowledge-base/ml-techniques"}},"content":"In very simple terms, Ito\'s lemma is a formula that allows us to compute the differential of a function of a stochastic process. In normal functions, we can use the chain rule to compute the differential of a function. However, in stochastic calculus, we need to use Ito\'s lemma.\\n\\n\x3c!-- truncate --\x3e\\n\\nSuppose we have a stochastic process $X_t$ that follows a stochastic differential equation (SDE).\\n\\n$$\\ndX_t = \\\\mu(X_t, t) dt + \\\\sigma(X_t, t) dW_t\\n$$\\n\\nwhere $W_t$ is a Wiener process (or Brownian motion), $\\\\mu(X_t, t)$ is the drift term, and $\\\\sigma(X_t, t)$ is the diffusion term.\\n\\nNow, let\'s say we have a function $f(X_t, t)$ (twice differentiable) that we want to differentiate with respect to time. Ito\'s lemma states that the differential of $f(X_t, t)$ is given by:\\n\\n$$\\n\\\\frac{\\\\Delta f(t)}{dt} dt\\n= \\\\frac{\\\\partial f}{\\\\partial t} dt\\n+ \\\\frac{1}{2}\\\\frac{\\\\partial^2f}{\\\\partial t^2} (dt)^2\\n+ \\\\cdots\\n$$\\n\\nand so with $x$, the total derivative of $f$ will be\\n\\n```math\\n\\\\begin{align*}\\ndf &= f_t dt + f_x dx \\\\\\\\\\n&=\\\\lim_{dx, dt\\\\rightarrow 0, 0} \\\\frac{\\\\partial f}{\\\\partial t} dt\\n+ \\\\frac{\\\\partial f}{\\\\partial x} dx\\n+ \\\\frac{1}{2} \\\\left(\\\\frac{\\\\partial^2f}{\\\\partial t^2} (dt)^2\\n+ \\\\frac{\\\\partial^2f}{\\\\partial x^2} (dx)^2 \\\\right)\\n+ \\\\cdots\\n\\\\end{align*}\\n```\\n\\nThan substitute $x = X_t$, in the limit $dt \\\\to 0$, the following terms tend to zero faster than $dt$ are:\\n\\n1. $(dt)^2$\\n\\n2. $dtdW_t$\\n\\n3. $(dx)^3$\\n\\nNoted that $(dB_t)^2 = \\\\mathcal{O}(dt)$ due to quadratic variation of a Wiener process. # TODO,\\nHance\\n\\n$$\\ndf\\n= \\\\lim_{dt \\\\to 0} \\\\left(\\\\frac{\\\\partial f}{\\\\partial t}\\n+ \\\\mu_t \\\\frac{\\\\partial f}{\\\\partial x}\\n+ \\\\frac{\\\\sigma_t^2}{2} \\\\frac{\\\\partial^2 f}{\\\\partial x^2} \\\\right) dt\\n+ \\\\sigma_t \\\\frac{\\\\partial f}{\\\\partial x} dW_t\\n$$\\n\\nThat\'s it!"},{"id":"ml-techniques","metadata":{"permalink":"/knowledge-base/ml-techniques","source":"@site/blog/2025-05-13-ml-techniques.md","title":"Machine Learning Techniques","description":"Notes to MLE interviews.","date":"2025-05-13T00:00:00.000Z","tags":[{"inline":false,"label":"Machine Learning","permalink":"/knowledge-base/tags/machine-learning","description":"Machine Learning"}],"readingTime":1.615,"hasTruncateMarker":true,"authors":[{"name":"Hinny Tsang","title":"Data Scientist @ Pollock Asset Management","url":"https://github.com/HinnyTsang","page":{"permalink":"/knowledge-base/authors/hinnytsang"},"socials":{"linkedin":"https://www.linkedin.com/in/HinnyTsang/","github":"https://github.com/HinnyTsang"},"imageURL":"https://github.com/HinnyTsang.png","key":"hinnytsang"}],"frontMatter":{"slug":"ml-techniques","title":"Machine Learning Techniques","authors":["hinnytsang"],"tags":["machine-learning"]},"unlisted":false,"prevItem":{"title":"Ito\'s Lamma","permalink":"/knowledge-base/ito-lamma"}},"content":"Notes to MLE interviews.\\n\\n\x3c!-- truncate --\x3e\\n\\n### One-Hot Encoding\\n\\nDoesn\'t work well with high cardinality categorical features, and tree-based models like XGBoost and LightGBM due to too many zeros.\\n\\n### Mean Encoding\\n\\nlet say sample is below.\\n\\n| Age | Income |\\n| --- | ------ |\\n| 18  | 60,000 |\\n| 18  | 50,000 |\\n| 18  | 40,000 |\\n| 19  | 66,000 |\\n| 19  | 51,000 |\\n| 19  | 42,000 |\\n\\nMean encoding encode age by the mean of income, i.e.\\n\\n| Age | Income | Mean Encoding |\\n| --- | ------ | ------------- |\\n| 18  | 60,000 | 50,000        |\\n| 18  | 50,000 | 50,000        |\\n| 18  | 40,000 | 50,000        |\\n| 19  | 66,000 | 53,000        |\\n| 19  | 51,000 | 53,000        |\\n| 19  | 42,000 | 53,000        |\\n\\nBe aware of label leakage. Can apply additive smoothing to make it more robust.\\n\\n### Feature Hashing\\n\\nMap to a fixed number of features. One problem is collision if hash size is too small.\\n\\n### Cross Feature\\n\\nJoin categorical features together. For example, if we have two categorical features, `A` and `B`, we can create a new feature `C` that is the concatenation of `A` and `B`. This can help capture interactions between the two features.\\n\\n### Embedding\\n\\nA way to represent categorical features as continuous vectors. This is often used in deep learning models, where we can learn the embeddings during training. Embeddings can capture complex relationships between categories and are particularly useful for high cardinality features.\\n\\n- Continuous Bag of Words (CBOW): words[t-n] to word[t-1], word[t+1] to word[t+n] to predict word[t]\\n- Skip-gram: word[t] to predict surrounding words.\\n\\n:::note\\n\\nRule of thumb: $ d = D4d = D^{1/4} $ where $D$ is the number of categories and $d$ is the dimension of the embedding.\\n\\n:::\\n\\n:::tip\\n\\nPre-trained word embeddings (ex. word2vec, word2glove, etc.) are powerful.\\n\\n:::"}]}}')}}]);